@article{smith-miles_cross-disciplinary_2009,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/1456650.1456656},
	doi = {10.1145/1456650.1456656},
	abstract = {The algorithm selection problem [Rice 1976] seeks to answer the question: Which algorithm is likely to perform best for my problem? Recognizing the problem as a learning task in the early 1990's, the machine learning community has developed the field of meta-learning, focused on learning about learning algorithm performance on classification problems. But there has been only limited generalization of these ideas beyond classification, and many related attempts have been made in other disciplines (such as AI and operations research) to tackle the algorithm selection problem in different ways, introducing different terminology, and overlooking the similarities of approaches. In this sense, there is much to be gained from a greater awareness of developments in meta-learning, and how these ideas can be generalized to learn about the behaviors of other (nonlearning) algorithms. In this article we present a unified framework for considering the algorithm selection problem as a learning problem, and use this framework to tie together the crossdisciplinary developments in tackling the algorithm selection problem. We discuss the generalization of meta-learning concepts to algorithms focused on tasks including sorting, forecasting, constraint satisfaction, and optimization, and the extension of these ideas to bioinformatics, cryptography, and other fields.},
	number = {1},
	urldate = {2021-06-24},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A.},
	month = jan,
	year = {2009},
	keywords = {Algorithm selection, classification, combinatorial optimization, constraint satisfaction, dataset characterization, empirical hardness, forecasting, landscape analysis, meta-learning, model selection, sorting},
	pages = {6:1--6:25},
}

@article{dhahri_automated_2019,
	title = {Automated {Breast} {Cancer} {Diagnosis} {Based} on {Machine} {Learning} {Algorithms}},
	volume = {2019},
	issn = {2040-2309},
	doi = {10.1155/2019/4253641},
	abstract = {There have been several empirical studies addressing breast cancer using machine learning and soft computing techniques. Many claim that their algorithms are faster, easier, or more accurate than others are. This study is based on genetic programming and machine learning algorithms that aim to construct a system to accurately differentiate between benign and malignant breast tumors. The aim of this study was to optimize the learning algorithm. In this context, we applied the genetic programming technique to select the best features and perfect parameter values of the machine learning classifiers. The performance of the proposed method was based on sensitivity, specificity, precision, accuracy, and the roc curves. The present study proves that genetic programming can automatically find the best model by combining feature preprocessing methods and classifier algorithms.},
	language = {eng},
	journal = {Journal of Healthcare Engineering},
	author = {Dhahri, Habib and Al Maghayreh, Eslam and Mahmood, Awais and Elkilani, Wail and Faisal Nagi, Mohammed},
	year = {2019},
	pmid = {31814951},
	pmcid = {PMC6878785},
	keywords = {Algorithms, Breast, Breast Neoplasms, Databases, Factual, Female, Humans, Image Interpretation, Computer-Assisted, Machine Learning, Reproducibility of Results, Sensitivity and Specificity},
	pages = {4253641},
	file = {Full Text:/home/davin/Zotero/storage/MQS5GBKF/Dhahri et al. - 2019 - Automated Breast Cancer Diagnosis Based on Machine.pdf:application/pdf},
}

@incollection{rice_algorithm_1976,
	title = {The {Algorithm} {Selection} {Problem}**{This} work was partially supported by the {National} {Science} {Foundation} through {Grant} {GP}-{32940X}. {This} chapter was presented as the {George} {E}. {Forsythe} {Memorial} {Lecture} at the {Computer} {Science} {Conference}, {February} 19, 1975, {Washington}, {D}. {C}.},
	volume = {15},
	url = {https://www.sciencedirect.com/science/article/pii/S0065245808605203},
	abstract = {The problem of selecting an effective algorithm arises in a wide variety of situations. This chapter starts with a discussion on abstract models: the basic model and associated problems, the model with selection based on features, and the model with variable performance criteria. One objective of this chapter is to explore the applicability of the approximation theory to the algorithm selection problem. There is an intimate relationship here and that the approximation theory forms an appropriate base upon which to develop a theory of algorithm selection methods. The approximation theory currently lacks much of the necessary machinery for the algorithm selection problem. There is a need to develop new results and apply known techniques to these new circumstances. The final pages of this chapter form a sort of appendix, which lists 15 specific open problems and questions in this area. There is a close relationship between the algorithm selection problem and the general optimization theory. This is not surprising since the approximation problem is a special form of the optimization problem. Most realistic algorithm selection problems are of moderate to high dimensionality and thus one should expect them to be quite complex. One consequence of this is that most straightforward approaches (even well-conceived ones) are likely to lead to enormous computations for the best selection. The single most important part of the solution of a selection problem is the appropriate choice of the form for selection mapping. It is here that theories give the least guidance and that the art of problem solving is most crucial.},
	language = {en},
	urldate = {2021-06-25},
	booktitle = {Advances in {Computers}},
	publisher = {Elsevier},
	author = {Rice, John R.},
	editor = {Rubinoff, Morris and Yovits, Marshall C.},
	month = jan,
	year = {1976},
	doi = {10.1016/S0065-2458(08)60520-3},
	pages = {65--118},
	file = {Submitted Version:/home/davin/Zotero/storage/BBANAZJA/Rice - 1976 - The Algorithm Selection ProblemThis work was par.pdf:application/pdf;ScienceDirect Snapshot:/home/davin/Zotero/storage/G5LW49SQ/S0065245808605203.html:text/html},
}

@article{wolpert_no_1997,
	title = {No free lunch theorems for optimization},
	volume = {1},
	issn = {1089778X},
	url = {http://ieeexplore.ieee.org/document/585893/},
	doi = {10.1109/4235.585893},
	abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of “no free lunch” (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori “head-to-head” minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems’ enforcing of a type of uniformity over all algorithms.},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Wolpert, D.H. and Macready, W.G.},
	month = apr,
	year = {1997},
	pages = {67--82},
	file = {Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf:/home/davin/Zotero/storage/EGLPDKTH/Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf:application/pdf},
}

@article{hospedales_meta-learning_2020,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.05439},
	abstract = {The ﬁeld of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a ﬁxed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We ﬁrst discuss deﬁnitions of meta-learning and position it with respect to related ﬁelds, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	language = {en},
	urldate = {2021-06-29},
	journal = {arXiv:2004.05439 [cs, stat]},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.05439},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:/home/davin/Zotero/storage/W98LLRXT/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf},
}

@article{claesen_hyperparameter_2015,
	title = {Hyperparameter {Search} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1502.02127},
	abstract = {We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.},
	urldate = {2021-07-19},
	journal = {arXiv:1502.02127 [cs, stat]},
	author = {Claesen, Marc and De Moor, Bart},
	month = apr,
	year = {2015},
	note = {arXiv: 1502.02127},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, G.1.6, I.2.6, I.2.8, I.5},
	annote = {Comment: 5 pages, accepted for MIC 2015: The XI Metaheuristics International Conference in Agadir, Morocco},
	file = {arXiv Fulltext PDF:/home/davin/Zotero/storage/LJJ86CL2/Claesen and De Moor - 2015 - Hyperparameter Search in Machine Learning.pdf:application/pdf;arXiv.org Snapshot:/home/davin/Zotero/storage/IQT3HL98/1502.html:text/html},
}

@article{wicaksono_hyper_2018,
	title = {Hyper {Parameter} {Optimization} using {Genetic} {Algorithm} on {Machine} {Learning} {Methods} for {Online} {News} {Popularity} {Prediction}},
	volume = {9},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=9&Issue=12&Code=ijacsa&SerialNo=38},
	doi = {10.14569/IJACSA.2018.091238},
	abstract = {Online news is a media for people to get new information. There are a lot of online news media out there and a many people will only read news that is interesting for them. This kind of news tends to be popular and will bring profit to the media owner. That’s why, it is necessary to predict whether a news is popular or not by using the prediction methods. Machine learning is one of the popular prediction methods we can use. In order to make a higher accuracy of prediction, the best hyper parameter of machine learning methods need to be determined. Determining the hyper parameter can be time consuming if we use grid search method because grid search is a method which tries all possible combination of hyper parameter. This is a problem because we need a quicker time to make a prediction of online news popularity. Hence, genetic algorithm is proposed as the alternative solution because genetic algorithm can get optimal hypermeter with reasonable time. The result of implementation shows that genetic algorithm can get the hyper parameter with almost the same result with grid search with faster computational time. The reduction in computational time is as follows: Support Vector Machine is 425.06\%, Random forest is 17\%, Adaptive Boosting is 651.06\%, and lastly K - Nearest Neighbour is 396.72\%.},
	language = {en},
	number = {12},
	urldate = {2021-08-20},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Wicaksono, Ananto Setyo and Afif, Ahmad},
	year = {2018},
	file = {Wicaksono and Afif - 2018 - Hyper Parameter Optimization using Genetic Algorit.pdf:/home/davin/Zotero/storage/NIG7I9Z2/Wicaksono and Afif - 2018 - Hyper Parameter Optimization using Genetic Algorit.pdf:application/pdf},
}

@article{li_genetic_nodate,
	title = {Genetic {Algorithm} based hyper-parameters optimization for transfer {Convolutional} {Neural} {Network}},
	abstract = {Hyperparameter optimization is a challenging problem in developing deep neural networks. Decision of transfer layers and trainable layers is a major task for design of the transfer convolutional neural networks (CNN). Conventional transfer CNN models are usually manually designed based on intuition. In this paper, a genetic algorithm is applied to select trainable layers of the transfer model. The filter criterion is constructed by accuracy and the counts of the trainable layers. The results show that the method is competent in this task. The system will converge with a precision of 97\% in the classification of Cats and Dogs datasets, in no more than 15 generations. Moreover, backward inference according the results of the genetic algorithm shows that our method can capture the gradient features in network layers, which plays a part on understanding of the transfer AI models.},
	language = {en},
	author = {Li, Chen and Jiang, JinZhe and Zhao, YaQian and Li, RenGang and Wang, EnDong and Zhang, Xin and Zhao, Kun},
	pages = {20},
	file = {Li et al. - Genetic Algorithm based hyper-parameters optimizat.pdf:/home/davin/Zotero/storage/QS24GFE7/Li et al. - Genetic Algorithm based hyper-parameters optimizat.pdf:application/pdf},
}

@article{xiao_efficient_2020,
	title = {Efficient {Hyperparameter} {Optimization} in {Deep} {Learning} {Using} a {Variable} {Length} {Genetic} {Algorithm}},
	url = {http://arxiv.org/abs/2006.12703},
	abstract = {Convolutional Neural Networks (CNN) have gained great success in many artiﬁcial intelligence tasks. However, ﬁnding a good set of hyperparameters for a CNN remains a challenging task. It usually takes an expert with deep knowledge, and trials and errors. Genetic algorithms have been used in hyperparameter optimizations. However, traditional genetic algorithms with ﬁxed-length chromosomes may not be a good ﬁt for optimizing deep learning hyperparameters, because deep learning models have variable number of hyperparameters depending on the model depth. As the depth increases, the number of hyperparameters grows exponentially, and searching becomes exponentially harder. It is important to have an efﬁcient algorithm that can ﬁnd a good model in reasonable time. In this article, we propose to use a variable length genetic algorithm (GA) to systematically and automatically tune the hyperparameters of a CNN to improve its performance. Experimental results show that our algorithm can ﬁnd good CNN hyperparameters efﬁciently. It is clear from our experiments that if more time is spent on optimizing the hyperparameters, better results could be achieved. Theoretically, if we had unlimited time and CPU power, we could ﬁnd the optimized hyperparameters and achieve the best results in the future.},
	language = {en},
	urldate = {2021-08-20},
	journal = {arXiv:2006.12703 [cs]},
	author = {Xiao, Xueli and Yan, Ming and Basodi, Sunitha and Ji, Chunyan and Pan, Yi},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12703},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Xiao et al. - 2020 - Efficient Hyperparameter Optimization in Deep Lear.pdf:/home/davin/Zotero/storage/5I7PPK6I/Xiao et al. - 2020 - Efficient Hyperparameter Optimization in Deep Lear.pdf:application/pdf},
}

@inproceedings{ferreira_applying_2020,
	address = {Glasgow, United Kingdom},
	title = {Applying {Genetic} {Programming} to {Improve} {Interpretability} in {Machine} {Learning} {Models}},
	isbn = {978-1-72816-929-3},
	url = {https://ieeexplore.ieee.org/document/9185620/},
	doi = {10.1109/CEC48606.2020.9185620},
	abstract = {Explainable Artiﬁcial Intelligence (or xAI) has become an important research topic in the ﬁelds of Machine Learning and Deep Learning. In this paper, we propose a Genetic Programming (GP) based approach, name Genetic Programming Explainer (GPX), to the problem of explaining decisions computed by AI systems. The method generates a noise set located in the neighborhood of the point of interest, whose prediction should be explained, and ﬁts a local explanation model for the analyzed sample. The tree structure generated by GPX provides a comprehensible analytical, possibly non-linear, expression which reﬂects the local behavior of the complex model. We considered three machine learning techniques that can be recognized as complex black-box models: Random Forest, Deep Neural Network and Support Vector Machine in twenty data sets for regression and classiﬁcations problems. Our results indicate that the GPX is able to produce more accurate understanding of complex models than the state of the art. The results validate the proposed approach as a novel way to deploy GP to improve interpretability.},
	language = {en},
	urldate = {2021-08-20},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Ferreira, Leonardo Augusto and Guimaraes, Frederico Gadelha and Silva, Rodrigo},
	month = jul,
	year = {2020},
	pages = {1--8},
	file = {Ferreira et al. - 2020 - Applying Genetic Programming to Improve Interpreta.pdf:/home/davin/Zotero/storage/9JEETVRB/Ferreira et al. - 2020 - Applying Genetic Programming to Improve Interpreta.pdf:application/pdf},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{scikit-learn-examples,
  title = {Scikit-learn classifier comparison demo example},
  howpublished = {\url{https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html}},
  note = {Accessed: 2021-09-01}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" } 

@misc{misc_breast_cancer_wisconsin_(diagnostic)_17,
  author       = {Wolberg, William and Street, W. and Mangasarian, Olvi},
  title        = {{Breast Cancer Wisconsin (Diagnostic)}},
  year         = {1995},
  howpublished = {UCI Machine Learning Repository}
}

@misc{see-segment ,
title = "SEE-Segment",
url = "https://github.com/see-insight/see-segment",
note = {Accessed: 2021-09-11} }

@misc{see-insight ,
title = "SEE-Insight",
url = "https://see-insight.github.io/",
note = {Accessed: 2021-09-11} }

%%


@inproceedings{feurer-neurips15a,
    title     = {Efficient and Robust Automated Machine Learning},
    author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina  Springenberg, Jost and Blum, Manuel and Hutter, Frank},
    booktitle = {Advances in Neural Information Processing Systems 28 (2015)},
    pages     = {2962--2970},
    year      = {2015}
}

@Inbook{Feurer2019,
author="Feurer, Matthias
and Hutter, Frank",
editor="Hutter, Frank
and Kotthoff, Lars
and Vanschoren, Joaquin",
title="Hyperparameter Optimization",
bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="3--33",
abstract="Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
isbn="978-3-030-05318-5",
doi="10.1007/978-3-030-05318-5_1",
url="https://doi.org/10.1007/978-3-030-05318-5_1"
}

@article{le2020scaling,
  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},
  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},
  journal={Bioinformatics},
  volume={36},
  number={1},
  pages={250--256},
  year={2020},
  publisher={Oxford University Press}
}

@article{amlb2019,
  title={An Open Source AutoML Benchmark},
  author={Gijsbers, P. and LeDell, E. and Poirier, S. and Thomas, J. and Bischl, B. and Vanschoren, J.},
  journal={arXiv preprint arXiv:1907.00909 [cs.LG]},
  url={https://arxiv.org/abs/1907.00909},
  note={Accepted at AutoML Workshop at ICML 2019},
  year={2019}
}

@misc{misc_blood_transfusion_service_center_176,
  author       = {Yeh, I-Cheng},
  title        = {{Blood Transfusion Service Center}},
  year         = {2008},
  howpublished = {UCI Machine Learning Repository}
}

@misc{Sayyad-Shirabad+Menzies:2005 ,
author = "Sayyad Shirabad, J. and Menzies, T.J.",
year = "2005",
title = "{The {PROMISE} Repository of Software Engineering Databases.}",
url = "http://promise.site.uottawa.ca/SERepository",
howpublished = "School of Information Technology and Engineering, University of Ottawa, Canada"}
